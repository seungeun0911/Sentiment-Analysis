{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a51dfbfb-0968-423e-a02e-4aca0e49dc40",
   "metadata": {},
   "source": [
    "## Movie_Review_Sentiment_Classification\n",
    "### (영화리뷰 감성분석)\n",
    "\n",
    "***\n",
    "#### 1. 데이터 준비와 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18594070-80a8-459a-85f3-d5ffd0fe071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "from eunjeon import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d50be0-9bd1-4931-94ff-ae734057645e",
   "metadata": {},
   "source": [
    "문장의 길이가 필터 사이즈보다 작으면 에러가 나므로 다음과 같이 토크나이저 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afe4fc9f-dcba-4fad-a62b-07a87f6ad77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_SIZES = [3,4,5]\n",
    "def tokenizer(text):\n",
    "    token = [t for t in mecab.morphs(text)]\n",
    "    if len(token) < max(FILTER_SIZES):\n",
    "        for i in range(0, max(FILTER_SIZES) - len(token)):\n",
    "            token.append('<PAD>')\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f49854af-b477-4749-913c-c6ffa9f82500",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize = tokenizer, batch_first = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b5e009c-d108-4edf-a7db-d304411e1c5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fields = {'document': ('text',TEXT), 'label': ('label',LABEL)}\n",
    "# dictionary 형식은 {csv컬럼명 : (데이터 컬럼명, Field이름)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cc4160e-e630-4a85-937d-6a7ee7e45c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"0112.csv\", encoding=\"utf-8\")\n",
    "df2 = pd.read_csv(\"0113.csv\", encoding=\"utf-8\")\n",
    "df3 = pd.read_csv(\"0114.csv\", encoding=\"utf-8\")\n",
    "df4 = pd.read_csv(\"0115.csv\", encoding=\"utf-8\")\n",
    "df5 = pd.read_csv(\"0116.csv\", encoding=\"utf-8\")\n",
    "df6 = pd.read_csv(\"0117.csv\", encoding=\"utf-8\")\n",
    "df7 = pd.read_csv(\"0118.csv\", encoding=\"utf-8\")\n",
    "df8 = pd.read_csv(\"0119.csv\", encoding=\"utf-8\")\n",
    "df9 = pd.read_csv(\"0120.csv\", encoding=\"utf-8\")\n",
    "df10 = pd.read_csv(\"0121.csv\", encoding=\"utf-8\")\n",
    "df11 = pd.read_csv(\"0122.csv\", encoding=\"utf-8\")\n",
    "df12 = pd.read_csv(\"0123.csv\", encoding=\"utf-8\")\n",
    "df13 = pd.read_csv(\"0124.csv\", encoding=\"utf-8\")\n",
    "df14 = pd.read_csv(\"0125.csv\", encoding=\"utf-8\")\n",
    "df15 = pd.read_csv(\"0126.csv\", encoding=\"utf-8\")\n",
    "df16 = pd.read_csv(\"0127.csv\", encoding=\"utf-8\")\n",
    "df17 = pd.read_csv(\"0128.csv\", encoding=\"utf-8\")\n",
    "df18 = pd.read_csv(\"0129.csv\", encoding=\"utf-8\")\n",
    "df19 = pd.read_csv(\"0130.csv\", encoding=\"utf-8\")\n",
    "df20 = pd.read_csv(\"0131.csv\", encoding=\"utf-8\")\n",
    "df21 = pd.read_csv(\"0201.csv\", encoding=\"utf-8\")\n",
    "df22 = pd.read_csv(\"0202.csv\", encoding=\"utf-8\")\n",
    "df23 = pd.read_csv(\"0203.csv\", encoding=\"utf-8\")\n",
    "df24 = pd.read_csv(\"0204.csv\", encoding=\"utf-8\")\n",
    "df25 = pd.read_csv(\"0205.csv\", encoding=\"utf-8\")\n",
    "df26 = pd.read_csv(\"0206.csv\", encoding=\"utf-8\")\n",
    "df27 = pd.read_csv(\"0207.csv\", encoding=\"utf-8\")\n",
    "df28 = pd.read_csv(\"0208.csv\", encoding=\"utf-8\")\n",
    "df29 = pd.read_csv(\"0209.csv\", encoding=\"utf-8\")\n",
    "df30 = pd.read_csv(\"0210.csv\", encoding=\"utf-8\")\n",
    "df31 = pd.read_csv(\"0211.csv\", encoding=\"utf-8\")\n",
    "df32 = pd.read_csv(\"0212.csv\", encoding=\"utf-8\")\n",
    "df33 = pd.read_csv(\"0213.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63ea50db-09b3-4414-bcff-5e2ed3baf82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27984\\3171784776.py:19: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['document'] = df['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27984\\3171784776.py:20: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['document'] = df['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,df17,df18,df19,df20,\n",
    "                df21,df22,df23,df24,df25,df26,df27,df28,df29,df30,df31,df32,df33], axis=0, ignore_index = True)\n",
    "df = df.rename(columns = {'sentence':'document'})\n",
    "\n",
    "def rating_to_label(score):\n",
    "    if score <= 2:\n",
    "        return 1\n",
    "    if 3 <= score <= 4:\n",
    "        return 2\n",
    "    if 5 <= score <= 6:\n",
    "        return 3\n",
    "    if 7 <= score <= 8:\n",
    "        return 4\n",
    "    if score >= 9:\n",
    "        return 5\n",
    "    \n",
    "df['label'] = df['score'].apply(lambda x: rating_to_label(x))\n",
    "df = df.drop(columns = 'score')\n",
    "df['document'] = df['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "df['document'] = df['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
    "df['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "df = df.dropna(axis = 0)\n",
    "df.set_index('document',inplace=True)\n",
    "df = df.astype('int')\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d93493e-df14-4682-a480-d78c7c196370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    43890\n",
       "4    12477\n",
       "1    10026\n",
       "3     6496\n",
       "2     3591\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39961045-5aed-45e6-84e3-6804b94e9679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>이걸 영화라고</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>절대 보지 마세요완전 지루하고 노잼입니다평점이 도저히 이해가 안갑니다돈이 너무 아깝...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그렇게   하고싶냐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>전개성은 비행기 이륙하는 순간 떨군 것 같아요 좋은 배우분들로 이런 영화를 찍은게 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>그냥 일본만화 ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  label\n",
       "0                                            이걸 영화라고      1\n",
       "1  절대 보지 마세요완전 지루하고 노잼입니다평점이 도저히 이해가 안갑니다돈이 너무 아깝...      1\n",
       "2                                         그렇게   하고싶냐      1\n",
       "3  전개성은 비행기 이륙하는 순간 떨군 것 같아요 좋은 배우분들로 이런 영화를 찍은게 ...      1\n",
       "4                                         그냥 일본만화 ㅋㅋ      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat((df[df['label']==1].head(len(df[df['label']==2])),df[df['label']==2],df[df['label']==3].head(len(df[df['label']==2])),\n",
    "                df[df['label']==4].head(len(df[df['label']==2])),df[df['label']==5].head(len(df[df['label']==2]))), axis=0, ignore_index = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a961880f-de0a-43bf-bfb1-0e2fe6f2105f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3591\n",
       "2    3591\n",
       "3    3591\n",
       "4    3591\n",
       "5    3591\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dd04e96-6779-4e99-a6a1-ea4301bc9712",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['은','는','이','가','의','들','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def preprocess(text):\n",
    "    word = [t for t in text if t not in stopwords]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bade12a9-0db3-4417-b5b9-21e03f07025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=0)\n",
    "\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4de291d7-224a-4536-a527-0b7f9fa4a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"train_data.csv\", index=False)\n",
    "test_data.to_csv(\"test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18250609-f37d-431d-97af-f11ba2d9715a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/14364 [00:00<?, ?it/s]C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27984\\3166225607.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data.append(stopwords_removed_sentence)\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 14364/14364 [00:31<00:00, 450.75it/s]\n"
     ]
    }
   ],
   "source": [
    "for sentence in tqdm(train_data['document']):\n",
    "    tokenized_sentence = mecab.morphs(sentence) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    train_data.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76e9f889-3558-47e3-94e2-ccaf2a1c9c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/3591 [00:00<?, ?it/s]C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27984\\2256332727.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  test_data.append(stopwords_removed_sentence)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3591/3591 [00:06<00:00, 571.21it/s]\n"
     ]
    }
   ],
   "source": [
    "for sentence in tqdm(test_data['document']):\n",
    "    tokenized_sentence = mecab.morphs(sentence) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    test_data.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1518b1e7-6f99-4254-9b54-58cc80f8c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train_data, test_data = data.TabularDataset.splits(\n",
    "                            path = '.',\n",
    "                            train = 'train_data.csv',\n",
    "                            test = 'test_data.csv',\n",
    "                            format = 'csv',\n",
    "                            fields = fields\n",
    ")\n",
    "train_data, valid_data = train_data.split(random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c1c2d4f-e248-4288-a880-e9eb2d353158",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "TEXT.build_vocab(train_data,\n",
    "                max_size = MAX_VOCAB_SIZE,\n",
    "                vectors = 'fasttext.simple.300d',\n",
    "                unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e1ad47f-2a1c-4653-923c-ce811eedd3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09c41c8a-f1f5-4ccc-ac66-7b89c144ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d068bdc8-0585-4f6a-9f4c-d8b639abb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shape(name, data):\n",
    "    print(f'{name} has shape {data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41525a86-86a6-4c9a-bd42-e432f3cc636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=1,\n",
    "                                             out_channels=n_filters,\n",
    "                                             kernel_size=(fs, embedding_dim))\n",
    "                                   for fs in filter_sizes])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        #print_shape('text', text)\n",
    "        # text = [batch_size, sent_len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        #print_shape('embedded', embedded)\n",
    "        # embedded = [batch_size, sent_len, emb_dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        #print_shape('embedded', embedded)\n",
    "        # embedded = [batch_size, 1, sent_len, emb_dim]\n",
    "        \n",
    "        #print_shape('self.convs[0](embedded)', self.convs[0](embedded))\n",
    "        # self.convs[0](embedded) = [batch_size, n_filters, sent_len-filter_sizes[n]+1, 1 ]\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        \n",
    "        #print_shape('F.max_pool1d(conved[0], conved[0].shape[2])', F.max_pool1d(conved[0], conved[0].shape[2]))\n",
    "        # F.max_pool1d(conved[0], conved[0].shape[2]) = [batch_size, n_filters, 1]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        #print_shape('cat', cat)\n",
    "        # cat = [batch_size, n_filters * len(filter_size)]\n",
    "        \n",
    "        res = self.fc(cat)\n",
    "        #print_shape('res', res)\n",
    "        # res = [batch_size, output_dim]\n",
    "        \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2be87e0f-cea2-4427-99f6-8979629bb067",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a2fff6b-8b74-444f-9fcc-daa09b89bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a090558-6334-4c62-9a56-40fad4ba9d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 파라미터 수는 4,548,001 개 입니다.\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'모델의 파라미터 수는 {count_parameters(model):,} 개 입니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fbca297-49f8-43ef-bf39-731247f5d163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13958, 300]) torch.Size([13958, 300])\n"
     ]
    }
   ],
   "source": [
    "pretrained_weight = TEXT.vocab.vectors\n",
    "print(pretrained_weight.shape, model.embedding.weight.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f161e31-7313-4fbb-9a19-ebb9f41c8667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ..., -1.4447,  0.8402, -0.8668],\n",
       "        [ 0.1032, -1.6268,  0.5729,  ...,  0.3180, -0.1626, -0.0417],\n",
       "        [-0.1020,  1.6282,  2.1635,  ..., -0.6009, -0.1467,  0.0285],\n",
       "        ...,\n",
       "        [ 0.2466,  0.7686, -0.8748,  ...,  1.2832, -0.6463,  0.3351],\n",
       "        [ 1.9161, -0.7881, -0.4273,  ..., -0.2970,  1.7449,  0.4935],\n",
       "        [ 0.8965, -0.0574,  0.0136,  ..., -1.6213, -0.2140,  0.5201]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "247e3307-f33c-4cae-ac87-9231db024e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be15e66-cea6-43d8-9a06-2efa5c4febef",
   "metadata": {},
   "source": [
    "## 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f56768c-bffa-480b-bd5b-a2caaa9b019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68921a48-36f7-41a9-a832-d0778ae3464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59fef0c5-fddb-47d0-a2d4-0da5a575574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds==y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9244054-c8b3-4d8f-ab6e-1858b2c8fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.text).squeeze(1) # output_dim = 1\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5376946f-5060-4a22-b855-7e6690500b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3e6c428-8cd6-47ad-a847-30c8a7fbd437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7b966e0-86d3-4bca-9b1f-06965ab4a432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 1/5 [00:15<01:03, 15.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: -547.130 | Train Acc: 20.08%\n",
      "\t Val. Loss: -2128.576 |  Val. Acc: 20.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:32<00:48, 16.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: -8910.389 | Train Acc: 20.00%\n",
      "\t Val. Loss: -20789.856 |  Val. Acc: 20.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:48<00:32, 16.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: -47590.695 | Train Acc: 20.00%\n",
      "\t Val. Loss: -85282.599 |  Val. Acc: 20.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [01:03<00:15, 15.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: -148046.317 | Train Acc: 20.00%\n",
      "\t Val. Loss: -229327.555 |  Val. Acc: 20.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:19<00:00, 15.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: -346242.571 | Train Acc: 20.08%\n",
      "\t Val. Loss: -490786.062 |  Val. Acc: 20.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "N_EPOCHS = 5\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(N_EPOCHS)):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut4-model.pt')\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52f3ca13-21c3-415b-bec1-e79ad308e09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: -502401.632 | Test Acc: 19.19%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut4-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb0257f5-d6d7-4463-afb4-88b0636cceb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 1/5 [00:17<01:09, 17.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: -671204.522 | Train Acc: 20.00%\n",
      "\t Val. Loss: -890532.848 |  Val. Acc: 20.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:35<00:54, 18.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: -1160749.428 | Train Acc: 20.00%\n",
      "\t Val. Loss: -1462038.785 |  Val. Acc: 20.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:55<00:37, 18.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Epoch Time: 0m 19s\n",
      "\tTrain Loss: -1805310.160 | Train Acc: 20.00%\n",
      "\t Val. Loss: -2211162.774 |  Val. Acc: 20.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [01:14<00:18, 18.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Epoch Time: 0m 19s\n",
      "\tTrain Loss: -2668784.336 | Train Acc: 20.00%\n",
      "\t Val. Loss: -3162471.789 |  Val. Acc: 20.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:34<00:00, 18.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Epoch Time: 0m 19s\n",
      "\tTrain Loss: -3714372.308 | Train Acc: 20.08%\n",
      "\t Val. Loss: -4322421.732 |  Val. Acc: 20.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(N_EPOCHS)):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut4-model.pt')\n",
    "            \n",
    "    print(f'Epoch: {epoch+6:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3996b20e-89b1-4d31-a73f-4e69e166a349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: -4406501.320 | Test Acc: 19.19%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut4-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geopandas",
   "language": "python",
   "name": "geopandas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
