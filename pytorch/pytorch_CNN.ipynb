{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a51dfbfb-0968-423e-a02e-4aca0e49dc40",
   "metadata": {},
   "source": [
    "## Movie_Review_Sentiment_Classification\n",
    "### (영화리뷰 감성분석)\n",
    "\n",
    "***\n",
    "#### 1. 데이터 준비와 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18594070-80a8-459a-85f3-d5ffd0fe071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "from eunjeon import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d50be0-9bd1-4931-94ff-ae734057645e",
   "metadata": {},
   "source": [
    "문장의 길이가 필터 사이즈보다 작으면 에러가 나므로 다음과 같이 토크나이저 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afe4fc9f-dcba-4fad-a62b-07a87f6ad77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_SIZES = [3,4,5]\n",
    "def tokenizer(text):\n",
    "    token = [t for t in mecab.morphs(text)]\n",
    "    if len(token) < max(FILTER_SIZES):\n",
    "        for i in range(0, max(FILTER_SIZES) - len(token)):\n",
    "            token.append('<PAD>')\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f49854af-b477-4749-913c-c6ffa9f82500",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize = tokenizer, batch_first = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b5e009c-d108-4edf-a7db-d304411e1c5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fields = {'document': ('text',TEXT), 'label': ('label',LABEL)}\n",
    "# dictionary 형식은 {csv컬럼명 : (데이터 컬럼명, Field이름)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cc4160e-e630-4a85-937d-6a7ee7e45c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"0112.csv\", encoding=\"utf-8\")\n",
    "df2 = pd.read_csv(\"0113.csv\", encoding=\"utf-8\")\n",
    "df3 = pd.read_csv(\"0114.csv\", encoding=\"utf-8\")\n",
    "df4 = pd.read_csv(\"0115.csv\", encoding=\"utf-8\")\n",
    "df5 = pd.read_csv(\"0116.csv\", encoding=\"utf-8\")\n",
    "df6 = pd.read_csv(\"0117.csv\", encoding=\"utf-8\")\n",
    "df7 = pd.read_csv(\"0118.csv\", encoding=\"utf-8\")\n",
    "df8 = pd.read_csv(\"0119.csv\", encoding=\"utf-8\")\n",
    "df9 = pd.read_csv(\"0120.csv\", encoding=\"utf-8\")\n",
    "df10 = pd.read_csv(\"0121.csv\", encoding=\"utf-8\")\n",
    "df11 = pd.read_csv(\"0122.csv\", encoding=\"utf-8\")\n",
    "df12 = pd.read_csv(\"0123.csv\", encoding=\"utf-8\")\n",
    "df13 = pd.read_csv(\"0124.csv\", encoding=\"utf-8\")\n",
    "df14 = pd.read_csv(\"0125.csv\", encoding=\"utf-8\")\n",
    "df15 = pd.read_csv(\"0126.csv\", encoding=\"utf-8\")\n",
    "df16 = pd.read_csv(\"0127.csv\", encoding=\"utf-8\")\n",
    "df17 = pd.read_csv(\"0128.csv\", encoding=\"utf-8\")\n",
    "df18 = pd.read_csv(\"0129.csv\", encoding=\"utf-8\")\n",
    "df19 = pd.read_csv(\"0130.csv\", encoding=\"utf-8\")\n",
    "df20 = pd.read_csv(\"0131.csv\", encoding=\"utf-8\")\n",
    "df21 = pd.read_csv(\"0201.csv\", encoding=\"utf-8\")\n",
    "df22 = pd.read_csv(\"0202.csv\", encoding=\"utf-8\")\n",
    "df23 = pd.read_csv(\"0203.csv\", encoding=\"utf-8\")\n",
    "df24 = pd.read_csv(\"0204.csv\", encoding=\"utf-8\")\n",
    "df25 = pd.read_csv(\"0205.csv\", encoding=\"utf-8\")\n",
    "df26 = pd.read_csv(\"0206.csv\", encoding=\"utf-8\")\n",
    "df27 = pd.read_csv(\"0207.csv\", encoding=\"utf-8\")\n",
    "df28 = pd.read_csv(\"0208.csv\", encoding=\"utf-8\")\n",
    "df29 = pd.read_csv(\"0209.csv\", encoding=\"utf-8\")\n",
    "df30 = pd.read_csv(\"0210.csv\", encoding=\"utf-8\")\n",
    "df31 = pd.read_csv(\"0211.csv\", encoding=\"utf-8\")\n",
    "df32 = pd.read_csv(\"0212.csv\", encoding=\"utf-8\")\n",
    "df33 = pd.read_csv(\"0213.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63ea50db-09b3-4414-bcff-5e2ed3baf82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21752\\3171784776.py:19: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['document'] = df['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21752\\3171784776.py:20: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['document'] = df['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,df17,df18,df19,df20,\n",
    "                df21,df22,df23,df24,df25,df26,df27,df28,df29,df30,df31,df32,df33], axis=0, ignore_index = True)\n",
    "df = df.rename(columns = {'sentence':'document'})\n",
    "\n",
    "def rating_to_label(score):\n",
    "    if score <= 2:\n",
    "        return 1\n",
    "    if 3 <= score <= 4:\n",
    "        return 2\n",
    "    if 5 <= score <= 6:\n",
    "        return 3\n",
    "    if 7 <= score <= 8:\n",
    "        return 4\n",
    "    if score >= 9:\n",
    "        return 5\n",
    "    \n",
    "df['label'] = df['score'].apply(lambda x: rating_to_label(x))\n",
    "df = df.drop(columns = 'score')\n",
    "df['document'] = df['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "df['document'] = df['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
    "df['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "df = df.dropna(axis = 0)\n",
    "df.set_index('document',inplace=True)\n",
    "df = df.astype('int')\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d93493e-df14-4682-a480-d78c7c196370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    43890\n",
       "4    12477\n",
       "1    10026\n",
       "3     6496\n",
       "2     3591\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dd04e96-6779-4e99-a6a1-ea4301bc9712",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['은','는','이','가','의','들','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def preprocess(text):\n",
    "    word = [t for t in text if t not in stopwords]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bade12a9-0db3-4417-b5b9-21e03f07025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=0)\n",
    "\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4de291d7-224a-4536-a527-0b7f9fa4a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"train_data.csv\", index=False)\n",
    "test_data.to_csv(\"test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18250609-f37d-431d-97af-f11ba2d9715a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/61184 [00:00<?, ?it/s]C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21752\\3166225607.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data.append(stopwords_removed_sentence)\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 61184/61184 [05:03<00:00, 201.74it/s]\n"
     ]
    }
   ],
   "source": [
    "for sentence in tqdm(train_data['document']):\n",
    "    tokenized_sentence = mecab.morphs(sentence) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    train_data.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76e9f889-3558-47e3-94e2-ccaf2a1c9c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/15296 [00:00<?, ?it/s]C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21752\\2256332727.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  test_data.append(stopwords_removed_sentence)\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 15296/15296 [00:37<00:00, 410.47it/s]\n"
     ]
    }
   ],
   "source": [
    "for sentence in tqdm(test_data['document']):\n",
    "    tokenized_sentence = mecab.morphs(sentence) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    test_data.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1518b1e7-6f99-4254-9b54-58cc80f8c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train_data, test_data = data.TabularDataset.splits(\n",
    "                            path = '.',\n",
    "                            train = 'train_data.csv',\n",
    "                            test = 'test_data.csv',\n",
    "                            format = 'csv',\n",
    "                            fields = fields\n",
    ")\n",
    "train_data, valid_data = train_data.split(random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c1c2d4f-e248-4288-a880-e9eb2d353158",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "TEXT.build_vocab(train_data,\n",
    "                max_size = MAX_VOCAB_SIZE,\n",
    "                vectors = 'fasttext.simple.300d',\n",
    "                unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e1ad47f-2a1c-4653-923c-ce811eedd3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09c41c8a-f1f5-4ccc-ac66-7b89c144ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d068bdc8-0585-4f6a-9f4c-d8b639abb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shape(name, data):\n",
    "    print(f'{name} has shape {data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41525a86-86a6-4c9a-bd42-e432f3cc636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=1,\n",
    "                                             out_channels=n_filters,\n",
    "                                             kernel_size=(fs, embedding_dim))\n",
    "                                   for fs in filter_sizes])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        #print_shape('text', text)\n",
    "        # text = [batch_size, sent_len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        #print_shape('embedded', embedded)\n",
    "        # embedded = [batch_size, sent_len, emb_dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        #print_shape('embedded', embedded)\n",
    "        # embedded = [batch_size, 1, sent_len, emb_dim]\n",
    "        \n",
    "        #print_shape('self.convs[0](embedded)', self.convs[0](embedded))\n",
    "        # self.convs[0](embedded) = [batch_size, n_filters, sent_len-filter_sizes[n]+1, 1 ]\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        \n",
    "        #print_shape('F.max_pool1d(conved[0], conved[0].shape[2])', F.max_pool1d(conved[0], conved[0].shape[2]))\n",
    "        # F.max_pool1d(conved[0], conved[0].shape[2]) = [batch_size, n_filters, 1]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        #print_shape('cat', cat)\n",
    "        # cat = [batch_size, n_filters * len(filter_size)]\n",
    "        \n",
    "        res = self.fc(cat)\n",
    "        #print_shape('res', res)\n",
    "        # res = [batch_size, output_dim]\n",
    "        \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2be87e0f-cea2-4427-99f6-8979629bb067",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a2fff6b-8b74-444f-9fcc-daa09b89bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a090558-6334-4c62-9a56-40fad4ba9d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 파라미터 수는 7,861,201 개 입니다.\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'모델의 파라미터 수는 {count_parameters(model):,} 개 입니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fbca297-49f8-43ef-bf39-731247f5d163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25002, 300]) torch.Size([25002, 300])\n"
     ]
    }
   ],
   "source": [
    "pretrained_weight = TEXT.vocab.vectors\n",
    "print(pretrained_weight.shape, model.embedding.weight.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f161e31-7313-4fbb-9a19-ebb9f41c8667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ..., -1.4447,  0.8402, -0.8668],\n",
       "        [ 0.1032, -1.6268,  0.5729,  ...,  0.3180, -0.1626, -0.0417],\n",
       "        [-0.1020,  1.6282,  2.1635,  ..., -0.6009, -0.1467,  0.0285],\n",
       "        ...,\n",
       "        [-0.3387, -1.4635, -0.0609,  ...,  0.3799,  1.0763,  0.1709],\n",
       "        [ 1.1313,  0.7882, -0.9301,  ..., -1.6988,  1.0547, -1.1780],\n",
       "        [ 0.4711,  1.4491, -0.7185,  ..., -0.0707,  0.6123, -0.8139]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "247e3307-f33c-4cae-ac87-9231db024e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be15e66-cea6-43d8-9a06-2efa5c4febef",
   "metadata": {},
   "source": [
    "## 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f56768c-bffa-480b-bd5b-a2caaa9b019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68921a48-36f7-41a9-a832-d0778ae3464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59fef0c5-fddb-47d0-a2d4-0da5a575574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds==y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9244054-c8b3-4d8f-ab6e-1858b2c8fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.text).squeeze(1) # output_dim = 1\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5376946f-5060-4a22-b855-7e6690500b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3e6c428-8cd6-47ad-a847-30c8a7fbd437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7b966e0-86d3-4bca-9b1f-06965ab4a432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 1/5 [01:20<05:20, 80.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 1m 20s\n",
      "\tTrain Loss: -81.552 | Train Acc: 31.15%\n",
      "\t Val. Loss: -431.359 |  Val. Acc: 33.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▌                                                  | 2/5 [02:55<04:28, 89.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Epoch Time: 1m 35s\n",
      "\tTrain Loss: -2401.496 | Train Acc: 35.22%\n",
      "\t Val. Loss: -5987.214 |  Val. Acc: 34.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [04:35<03:07, 93.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Epoch Time: 1m 39s\n",
      "\tTrain Loss: -14070.523 | Train Acc: 35.89%\n",
      "\t Val. Loss: -24944.217 |  Val. Acc: 33.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [06:19<01:38, 98.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Epoch Time: 1m 44s\n",
      "\tTrain Loss: -43673.849 | Train Acc: 35.94%\n",
      "\t Val. Loss: -65008.408 |  Val. Acc: 34.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [08:03<00:00, 96.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Epoch Time: 1m 43s\n",
      "\tTrain Loss: -98131.239 | Train Acc: 36.28%\n",
      "\t Val. Loss: -131500.562 |  Val. Acc: 34.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "N_EPOCHS = 5\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(N_EPOCHS)):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut4-model.pt')\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52f3ca13-21c3-415b-bec1-e79ad308e09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: -130895.431 | Test Acc: 35.37%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut4-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb0257f5-d6d7-4463-afb4-88b0636cceb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 1/5 [01:44<06:56, 104.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Epoch Time: 1m 44s\n",
      "\tTrain Loss: -183838.562 | Train Acc: 36.26%\n",
      "\t Val. Loss: -229074.940 |  Val. Acc: 34.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 2/5 [03:25<05:07, 102.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Epoch Time: 1m 41s\n",
      "\tTrain Loss: -302592.981 | Train Acc: 36.31%\n",
      "\t Val. Loss: -361909.386 |  Val. Acc: 34.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 3/5 [05:10<03:26, 103.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Epoch Time: 1m 44s\n",
      "\tTrain Loss: -464330.699 | Train Acc: 36.41%\n",
      "\t Val. Loss: -535287.599 |  Val. Acc: 34.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████▍                | 4/5 [06:53<01:43, 103.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Epoch Time: 1m 43s\n",
      "\tTrain Loss: -667862.035 | Train Acc: 36.19%\n",
      "\t Val. Loss: -751199.865 |  Val. Acc: 35.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [08:38<00:00, 103.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Epoch Time: 1m 44s\n",
      "\tTrain Loss: -920719.567 | Train Acc: 36.65%\n",
      "\t Val. Loss: -1015213.235 |  Val. Acc: 34.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(N_EPOCHS)):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut4-model.pt')\n",
    "            \n",
    "    print(f'Epoch: {epoch+6:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3996b20e-89b1-4d31-a73f-4e69e166a349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: -1011057.261 | Test Acc: 35.38%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut4-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geopandas",
   "language": "python",
   "name": "geopandas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
